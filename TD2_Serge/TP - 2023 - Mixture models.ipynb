{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759ac9ac",
   "metadata": {},
   "source": [
    "# Practical work - Mixing models\n",
    "\n",
    "Useful external references :\n",
    "\n",
    "> - [NumPy documentation](https://docs.scipy.org/doc/numpy/user/index.html)  \n",
    "- Documentation SciPy](https://docs.scipy.org/doc/scipy/reference/)  \n",
    "- Documentation MatPlotLib](http://matplotlib.org/)  \n",
    "- Scikit-learn website](http://scikit-learn.org/stable/index.html)  \n",
    "- Site langage python](https://www.python.org)  \n",
    "\n",
    "\n",
    "\n",
    "**The aim** of this tutorial is to introduce the use of Scikit-learn's features for estimating mixture models,\n",
    "and to contribute to a better understanding of this method and of techniques for choosing the number of components.\n",
    "To this end, we first examine data generated in a controlled way, and then real data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea397b6a",
   "metadata": {},
   "source": [
    "## Gaussian mixture models\n",
    "\n",
    "\n",
    "A presentation of the tools available for mixture models can be found in [http://scikit-learn.org/stable/modules/mixture.html](http://scikit-learn.org/stable/modules/mixture.html).\n",
    "\n",
    "\n",
    "Model selection (the choice of the number of mixture components, but also the type of covariance matrix between `'full'`, `'tied'`, `'diag'`, `'spherical'`, see below) can be made by calculating *Akaike information criterion* (AIC) or *Bayes information criterion* (BIC).\n",
    "\n",
    "\n",
    "Among the parameters, let's focus on the following (see [implementation description](http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) for the others):\n",
    "\n",
    "\n",
    "- `n_components`: the number of components in the mixture (`1` by default).  \n",
    "- `covariance_type` : type of covariance matrix used, choice between `'full'`, `'tied'`, `'diag'` and `'spherical'`, default `'full'` ; `'full'` = any matrix, `'tied'` = any matrix but identical between the different components of the mixture, `'diag'` = diagonal matrix (any variances but zero covariances), `'spherical'` = each component has its own variance (but this value is shared by all variables for that component) and zero covariances.  \n",
    "- `init_params`: parameter initialization method, choice between `'kmeans'` (automatic classification of observations with *K-means*, then use of each group center) and `'kmeans'` (automatic classification of observations with *K-means*, then use of each group center).\n",
    "- `n_init`: number of initializations (followed by EM executions) performed; the best result is retained.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Among the accessible attributes we can mention (see the documentation for the full list):\n",
    "\n",
    "\n",
    "- `weights_`: the mixing coefficients (weights of the different components of the mixture).  \n",
    "- `Means_`: the averages of the components.  \n",
    "- `covariances_`: the covariance matrices of the components.  \n",
    "- `converged_` : `True` if EM has converged (in `.fit()`), `False` otherwise.  \n",
    "- `lower_bound_`: log-likelihood achieved at the end of iterations by EM's best execution.  \n",
    "\n",
    "\n",
    "- `fit(X, y=None)`: calculation of the model from the observations which are the lines of X.  \n",
    "- aic(X)`: value of the Akaike information criterion on the data of X for the current model.  \n",
    "- `bic(X)`: value of the Bayes information criterion on the data from X for the current model.  \n",
    "- `predict_proba(X)`: *a posteriori* probabilities with respect to each component of the current model for each item of X data.  \n",
    "- `predict(X, y=None)`: group labels (automatic classification from the mixture model using *a posteriori* probabilities) obtained with the current model for $X$ data.  \n",
    "- `sample([n_samples, random_state])`: generate a sample (composed of `n_samples` data) from the model (for the moment, accessible only for kernels\n",
    "\n",
    "Methods that can be used:\n",
    "\n",
    "- `fit(X, y=None)`: calculation of the model from the observations which are the lines of X.  \n",
    "- aic(X)`: value of the Akaike information criterion on the data of X for the current model.  \n",
    "- `bic(X)`: value of the Bayes information criterion on the data from X for the current model.  \n",
    "- `predict_proba(X)`: *a posteriori* probabilities with respect to each component of the current model for each item of X data.\n",
    "- `predict(X, y=None)`: group labels (automatic classification from the mixture model using *a posteriori* probabilities) obtained with the current model for X data.  \n",
    "- `sample([n_samples, random_state])`: generate a sample (composed of `n_samples` data) from the model (currently only available for `'gaussian'` and `'tophat'` kernels).  \n",
    "- `score(X, y=None)`: returns the total log-likelihood of X data with respect to the model.  \n",
    "- `score_samples(X)`: returns the logarithm of the density calculated for each X data item.  \n",
    "- `get_params([deep])`: read the parameter values of the estimator used.  \n",
    "- `set_params(**params)`: give values to the parameters of the estimator used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6f096",
   "metadata": {},
   "source": [
    "### Estimation from generated data\n",
    "\n",
    "A first use on one-dimensional data (note that the `GaussianMixture` class is available from Scikit-learn version 0.18 onwards; for earlier versions, use the `GMM` class instead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd486521",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f7e83d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# sample with  \\pi_1=0.3 and \\pi_2=0.7 \n",
    "N = 100\n",
    "X = np.concatenate((np.random.normal(0, 1, int(0.3 * N)),\n",
    "                    np.random.normal(5, 1, int(0.7 * N))))[:, np.newaxis]\n",
    "\n",
    "# prepare the data\n",
    "X_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\n",
    "\n",
    "true_density = (0.3*norm(0,1).pdf(X_plot[:,0]) + 0.7*norm(5,1).pdf(X_plot[:,0]))\n",
    "\n",
    "# estimation using the correct number of component\n",
    "gmm = GaussianMixture(n_components=2,n_init=3).fit(X)\n",
    "print(gmm.converged_)\n",
    "print(gmm.lower_bound_)\n",
    "\n",
    "# compute the density\n",
    "density = np.exp(gmm.score_samples(X_plot))\n",
    "\n",
    "# Plot true density and estimated density\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "ax.fill_between(X_plot[:,0], true_density, fc='b', alpha=0.2, label='Vraie densité')\n",
    "ax.plot(X_plot[:,0], density, '-', label=\"Estimation\")\n",
    "ax.plot(X[:, 0], -0.005 - 0.01 * np.random.random(X.shape[0]), '+k')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8816114d",
   "metadata": {},
   "source": [
    "### Todo:\n",
    "\n",
    "Vary the number of components and visually examine the results. Look at the component weights and averages. Examine how the final value reached by the log-likelihood evolves with the number of components.\n",
    "\n",
    "Next, we test on two-dimensional data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9174e84",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Sample three clusters \n",
    "md1 = 1.5 * np.random.randn(200,2) + [3,3]\n",
    "md2 = np.random.randn(100,2).dot([[2, 0],[0, 0.8]]) + [-3, 3]\n",
    "md3 = np.random.randn(100,2) + [3, -3]\n",
    "md = np.concatenate((md1, md2, md3))\n",
    "\n",
    "# data for density\n",
    "grid_size = 100\n",
    "Gx = np.arange(-10, 10, 20/grid_size)\n",
    "Gy = np.arange(-10, 10, 20/grid_size)\n",
    "Gx, Gy = np.meshgrid(Gx, Gy)\n",
    "\n",
    "# estimation of the gmm\n",
    "gmm = GaussianMixture(n_components=3,n_init=3).fit(md)\n",
    "\n",
    "# compute true model\n",
    "density = np.exp(gmm.score_samples(np.hstack(((Gx.reshape(grid_size*grid_size))[:,np.newaxis],\n",
    "        (Gy.reshape(grid_size*grid_size)[:,np.newaxis])))))\n",
    "\n",
    "# show 3d results\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(16,8))\n",
    "ax.plot_surface(Gx, Gy, density.reshape(grid_size,grid_size), rstride=1,\n",
    "                    cstride=1, cmap=cm.coolwarm, linewidth=0, antialiased=True)\n",
    "ax.scatter(md[:,0], md[:,1], -0.025)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b946bbf1",
   "metadata": {},
   "source": [
    "### Todo:\n",
    "\n",
    "Vary the number of components and examine the results visually. See how the final log-likelihood value changes with the number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade55481",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61624771",
   "metadata": {},
   "source": [
    "### Todo:\n",
    "\n",
    "Generate two-dimensional data following a **uniform** distribution in $[0, 1]^2 $ (two-dimensional data in the unit square). Estimate a Gaussian mixture with 3 components, using `n_init = 1`. Visualize the results. Use the `predict` method of `GaussianMixture` to obtain group labels for the data. Apply the modeling followed by group label assignment several times in succession and examine the **stability of the partitionings** using [the adjusted Rand index](http://scikit-learn.org/stable/modules/clustering.html#adjusted-rand-index), as in automatic classification practical exercises.\n",
    "\n",
    "Note that you don't have any groups defined at the outset; to define reference groups, to which you'll compare those derived from other classifications, you can first apply density estimation () followed by group label assignment with `predict`. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d3f1c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bb89f3a",
   "metadata": {},
   "source": [
    "### Choosing the number of components and type of covariance matrix for the generated data\n",
    "\n",
    "To choose the number of components in the mixture, we first compare the AIC and BIC criteria, using `'full'` (default) covariance matrices. On generated two-dimensional data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7154ba76",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "n_max = 8    # maximal number of components\n",
    "n_components_range = np.arange(n_max)+1\n",
    "aic = []\n",
    "bic = []\n",
    "\n",
    "# estimate models and get aic and bic values\n",
    "for n_comp in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n_comp).fit(md)\n",
    "    aic.append(gmm.aic(md))\n",
    "    bic.append(gmm.bic(md))\n",
    "\n",
    "print(\"AIC : \" + str(aic))\n",
    "print(\"BIC : \" + str(bic))\n",
    "\n",
    "# normalization\n",
    "raic = aic/np.max(aic)\n",
    "rbic = bic/np.max(bic)\n",
    "\n",
    "# bar plots\n",
    "xpos = np.arange(n_max)+1  # bar localization\n",
    "larg = 0.3              # largeur des barres\n",
    "fig,ax = plt.subplots(1,1,figsize=(16,8))\n",
    "ax.set_ylim([min(np.concatenate((rbic,raic)))-0.1, 1.1])\n",
    "ax.set_xlabel('Nombre de composantes')\n",
    "ax.set_ylabel('Score')\n",
    "ax.bar(xpos, raic, larg, color='r', label=\"AIC\")\n",
    "ax.bar(xpos+larg, rbic, larg, color='b', label=\"BIC\")\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2628ea3",
   "metadata": {},
   "source": [
    "For these data and with `'full'` covariance matrices, both AIC and BIC favor the use of 3 components, a number equal to that of the normal distributions used to generate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076a558d",
   "metadata": {},
   "source": [
    "### Todo:\n",
    "\n",
    "Perform the same comparison for these data with `'diag'` covariance matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29590a6e",
   "metadata": {},
   "source": [
    "### Todo:\n",
    "\n",
    "Add on the same (adapted) graph the normalized values of the final log-likelihood for each value of `n_components`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8229338",
   "metadata": {},
   "source": [
    "### Estimation from texture data\n",
    "\n",
    "We will apply Gaussian mixture density estimation to texture data projected on the first two principal axes. These data correspond to 5,500 observations described by 40 variables. Each observation belongs to one of 11 texture classes; each class is represented by 500 observations.\n",
    "The data are taken from https://sci2s.ugr.es/keel/dataset.php?cod=72\n",
    "\n",
    "The data can be searched on the Moodle server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89483a2",
   "metadata": {},
   "source": [
    "The data are then analyzed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cea4f9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "textures = np.loadtxt('./input/texture.dat')\n",
    "pca = PCA().fit(textures[:,:40])\n",
    "texturesp = pca.transform(textures[:,:40])\n",
    "\n",
    "# build gmm on pca components\n",
    "gmm = GaussianMixture(n_components=11,n_init=3).fit(texturesp[:,:2])\n",
    "print(gmm.converged_)\n",
    "\n",
    "# True\n",
    "print(gmm.n_iter_)\n",
    "# 10\n",
    "print(gmm.lower_bound_)\n",
    "# -1.7780023505669722\n",
    "\n",
    "# data for the density\n",
    "grid_size = 100\n",
    "xmin = 1.3*np.min(texturesp[:,0])\n",
    "xmax = 1.3*np.max(texturesp[:,0])\n",
    "Gx = np.arange(xmin, xmax, (xmax-xmin)/grid_size)\n",
    "ymin = 1.3*np.min(texturesp[:,1])\n",
    "ymax = 1.3*np.max(texturesp[:,1])\n",
    "Gy = np.arange(ymin, ymax, (ymax-ymin)/grid_size)\n",
    "Gx, Gy = np.meshgrid(Gx, Gy)\n",
    "\n",
    "# compute density\n",
    "density = np.exp(gmm.score_samples(np.hstack(((Gx.reshape(grid_size*grid_size))[:,np.newaxis],\n",
    "                                   (Gy.reshape(grid_size*grid_size)[:,np.newaxis])))))\n",
    "\n",
    "# plots\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(16, 8))\n",
    "ax.plot_surface(Gx, Gy, density.reshape(grid_size,grid_size), rstride=1,\n",
    "                    cstride=1, cmap=cm.coolwarm, linewidth=0, antialiased=True)\n",
    "ax.scatter(texturesp[:,0], texturesp[:,1], -0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab8252",
   "metadata": {},
   "source": [
    "### Todo:\n",
    "\n",
    "Apply Gaussian mixture density estimation to \"texture\" data projected on the first two **discriminant** axes (obtianed by LDA). Increment the number of discriminant axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2eb229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "222f2e79",
   "metadata": {},
   "source": [
    "### Choice of number of components and type of covariance matrix for texture data\n",
    "\n",
    "First, let's determine the best number of components using the AIC and BIC criteria for projections of these data onto the first two principal axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11c5e3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "n_max = 10    # maximal number of components\n",
    "n_components_range = np.arange(n_max)+6\n",
    "aic = []\n",
    "bic = []\n",
    "\n",
    "# build models and get critria\n",
    "for n_comp in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n_comp).fit(texturesp[:,:2])\n",
    "    aic.append(gmm.aic(texturesp[:,:2]))\n",
    "    bic.append(gmm.bic(texturesp[:,:2]))\n",
    "\n",
    "\n",
    "# normalize results\n",
    "raic = aic/np.max(aic)\n",
    "rbic = bic/np.max(bic)\n",
    "\n",
    "# bar plot\n",
    "xpos = n_components_range  # bar localization\n",
    "larg = 0.3\n",
    "fig = plt.figure()\n",
    "plt.ylim([min(np.concatenate((rbic,raic)))-0.02, 1.01])\n",
    "plt.bar(xpos, raic, larg, color='r', label=\"AIC\")\n",
    "plt.bar(xpos+larg, rbic, larg, color='b', label=\"BIC\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0640e1",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "How do you explain that the optimal value obtained with BIC for `n_components` is less than the number of texture classes (which is 11)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b48fc",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "Carry out the same study using instead the projections of the \"texture\" data on the first two **discriminating** axes. How do you explain the difference compared to the previous case?"
   ]
  }
 ],
 "metadata": {
  "date": 1700728000.30339,
  "filename": "tpModelesMelange.rst",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "title": "Travaux pratiques - Modèles de mélange"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
